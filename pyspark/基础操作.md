# 配置



```python
from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession
sc=SparkContext()
spark = SparkSession(sparkContext=sc)

# 添加python文件
sc.addPyFile('pyFiles/my_module.py')
SparkFiles.get('my_module.py') # 获取
from my_module import * # 然后就可以使用了



```





# 创建

## RDD

```python
sc.parallelize([('cat', 'dog', 'fish'), ('orange', 'apple')]).collect() # collect返回list of Row（相当于没有parallelize的行对象）
# [('cat', 'dog', 'fish'), ('orange', 'apple')]

rdd = sc.textFile('../../data/mtcars.csv').take(2)
# [',mpg,cyl,disp,hp,drat,wt,qsec,vs,am,gear,carb',
# 'Mazda RX4,21,6,160,110,3.9,2.62,16.46,0,1,4,4',]

df.rdd # df转rdd
```



## DataFrame

```python
mtcars = spark.read.csv(path='../../data/mtcars.csv') # csv
df = spark.createDataFrame(pdf) # pandas.DataFrame
df = spark.createDataFrame([['a', 1], ['b', 2]], ['letter', 'number']) # list

# rdd转df
from pyspark.sql import Row
def list_to_row(keys, values):
    row_dict = dict(zip(keys, values))
    return Row(**row_dict)
rdd_rows = rdd.map(lambda x: list_to_row(header, x))
df = spark.createDataFrame(rdd_rows)

# 选取列
df = df.select(['col1', 'col2'])
df.col1

# 添加一列
mtcars = mtcars.withColumn('bbb', array_col)

# 保存
from pyspark.sql import DataFrameWriter
mtcars = mtcars.coalesce(numPartitions=1) # 先放到一个partition里
mtcars.write.csv('data/saved-mtcars', header=True) # 再写入文件
# twitter.saveAsTextFile('data/saved-twitter')
```



# 各种操作

## RDD

```python
rdd.map(lambda x: x.split(','))
rdd.mapValues(lambda x: np.mean(x)) # 对(key, value)的value起效果，如(1, (1,2,3))
rdd.flatMap(lambda x: x) # 先flatten，再map
rdd.flatMapValues(lambda x: list(zip(list('ABC'), x))) # 只对value，先flatten再map

# aggregate 计算平方和，rdd: mpg和disp列
zeroValue = (0, 0) # 这里需要两个中间变量
# seqOp：z代表每一轮得到的结果，x代表rdd中的一个partition的元素，所以他们都是zerovalue样式的二维tuple
seqOp = lambda z, x: (z[0] + x[0]**2, z[1] + x[1]**2)
# combOp：两者都是返回的结果，可能来自不同的partition，因此需要各自结果合并
combOp = lambda px, py: ( px[0] + py[0], px[1] + py[1] )
rdd.aggregate(zeroValue, seqOp, combOp)
# aggregateByKey 对不同的key享用不同的聚合，groupby的思路

# zipWithIndex 把rdd跟他们的index放入一个tuple(Row,index)
mtcars.rdd.zipWithIndex().take(5)
```



## DataFrame

```python
# filter
df.filter(df.index.isin([1,3,4])).show()

# index
df.index

# pyspark.sql.Column.xxx   bool
df.index.isin([1,3,4]) # 返回的是一列True/False
df.index.between(5,10)
df.col2 < 9
df.col2.contains(9)
df.col2.endswith('t') # startswith()
df.col2.isNotNull() # isNull()
df.col2.like('Ho%') # SQL LIKE
df.col2.rlike('t$') # SQL RLIKE (LIKE with Regex)

# F.when
from pyspark.sql import functions as F
filtering_column = F.when((mtcars_df.vs == 1) & (mtcars_df.am == 1), 1).name('filter_col') # 有点像np.where

# F.abs
F.abs(df.x)
F.concat(df.a, df.b) # 合成一列，比如字符串合并
F.corr(df.a, df.b)
F.array([df.a,df.b]) # 合成一列

# 删除列
df.drop('filter_col')

# 重命名
df.withColumnRenamed('name', 'new_name')

# 排序
df.select(df.name).orderBy(df.name.asc()) # asc_nulls_first()
```



# EDA

## RDD



## DataFrame

```python
nrow = df.count() # 行数
for col_name in df.columns:
	df.select(col_name).describe() # count、mean、std、min、max

```







# 数据预处理

## RDD





## DataFrame

```python
# pipeline
from pyspark.ml import Pipeline
pipeline = Pipeline(stages=[binarizer, bucketizer])
pipeline.fit(df).transform(df).show()

# 二值化、bin化、onehot（返回稀疏矩阵）
from pyspark.ml.feature import Binarizer, Bucketizer, OneHotEncoder
binarizer = Binarizer(threshold=0, inputCol='x1', outputCol='x1_new') # 0,1二值化
bucketizer = Bucketizer(splits=[0, 2.5, 5, 7.5, 10], inputCol='x2', outputCol='x2_new') # bin化
OneHotEncoder(dropLast=False, inputCol='x1', outputCol='encoded_x1').transform(df) # 3个unique值就是3列，dropLast

# label_encoder 按照词频排序
from pyspark.ml.feature import StringIndexer
indexer = StringIndexer(inputCol='x1', outputCol='indexed_x1')
indexer.fit(df).transform(df).show()

# 把多列合成一个（稀疏）矩阵
VectorAssembler(inputCols=['ohe_x1', 'ohe_x2', 'ohe_x3', 'x4'], outputCol='featuresCol').transform(df_new)

# udf  user defined function
from pyspark.sql.types import *
from pyspark.sql.functions import udf
my_udf = udf(lambda a,b:a/b, returnType=FloatType())
df.select(my_udf(df.a,df.b))
# return ArrayType
my_udf = udf(lambda a,b:[float(a),float(b)], returnType=ArrayType(FloatType()))
df.select(my_udf(df.a,df.b))
# return StructType
my_udf = udf(lambda a,b:[float(a),float(b)], returnType=StructType([StructField('f1', StringType()), StructField('f2': FloatType())])
df.select(my_udf(df.a,df.b))


```



# ML

## Models





## others



























