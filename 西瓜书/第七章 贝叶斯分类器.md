[EM算法](https://www.jianshu.com/p/1121509ac1dc)

## 贝叶斯决策论
$\lambda_ij$是将j类的样本误分类到i类的损失。基于后验概率$P(c_i|x)$可获得将样本x分类为$c_i$的期望损失，即在样本x上的条件风险：
$$
R(c_i|x) = \sum_{j=1}^N \lambda_{ij}P(c_j|x)

R(c|x) = 1-P(c|x)
$$
于是，最小化分类错误率的贝叶斯最优分类器为
$$
h^*(x)=\arg\max_{c \in y} P(c|x)
$$
**贝叶斯定理**：$P(c|\mathbf{x}) = \frac{P(\mathbf{x},c)}{P(\mathbf{x})} = \frac{P(c)P(x|c)}{P(x)}$
,P(x|c) 通过极大似然估计得到。


## 极大似然估计
Maximum Likelihood Estimation (MLE)

$D_c$为第c类样本组成的集合，假设样本是独立同分布的，那么参数$\theta_c$对于数据集的似然是：
$$
P(D_c|\theta_c) = \prod_{\mathbf{x}\in D_c}P(\mathbf{x}|\theta_c)
$$
因为该式的连乘容易造成下溢，通常使用对数似然(log-likelihood)
$$
LL(\theta_c) = logP(D_c|\theta_c)=\sum_{\mathbf{x}\in D_c}logP(\mathbf{x}|\theta_c)
$$
**注意**：此方法严重依赖于假设的概率分布。

## 朴素贝叶斯分类器
采用"属性条件**独立性**假设"，假设每个属性独立地对分类结果发生影响。因此可式子可重写为：
$$
P(c|x) = \frac{P(c)P(x|c)}{P(x)}=\frac{P(c)}{P(x)} \prod_{i=1}^d P(x_i|c) \text{ ，其中d为属性数目}
\\
\text{由于}P(x)\text{固定不变，可以忽略。}
\\
\text{朴素贝叶斯分类器的表达式：}h_{nb}(x)=\arg \max_{c \in y} P(c) \prod_{i=1}^d P(x_i|c)
$$
对$P(x_i|c)$，离散属性直接统计计算，连续属性计算样本均值和方差，假设服从正态分布，并计算概率密度函数。
$$
p(c_i|x) = 
    \begin{cases}
    \frac{|D_{c,x_i}|}{|D_c|} & \text{，离散} \\
    \frac{1}{\sqrt{2\pi}\sigma_{c,i}}exp(-\frac{(x_i-\mu_{c,i})^2}{2\sigma_{c,i}^2}) & \text{，连续}
    \end{cases}
$$
对于连续值的情况，要么划分区间，把它变为离散值。要么假设它服从正态（或均匀）分布，统计出均值方差，然后对于每一个$p(x_i|c)$都带入它的$x_i$来求解。

然而若是某一种属性的某一个值未在样本中出现，概率为0，会抹去其他属性的作用，因此要“平滑”，常用“拉普拉斯修正”。令N表示训练集D中可能的类别数，$N_i$表示第i个属性可能的取值数(假设属性值与类别均匀分布)。修正为：
$$
\hat{P}(c) = \frac{|D_c|+1}{|D|+N}
\\
\hat{P}(x_i|c) = \frac{|D_{c,x_i}|+1}{|D_c|+N_i}
$$
## 半朴素贝叶斯分类器
“独依赖估计”：对于第i个属性而言，找到它的父属性，求在父属性的情况下它的概率。其中父属性通过交叉验证等方法求出。

- **SPODE**方法假设所有属性依赖于同一属性，那么只需要交叉验证n-1次。
- **TAN**方法：计算两两属性间的条件互信息，并构造最大带权有向生成树，以此为父属性。
$$
I(x_i,x_j|y) = \sum_{x_i,x_j;c \in y} P(x_i,x_j|c)log\frac{P(x_i,x_j|c)}{P(x_i|c)P(x_j|c)}
$$
- AODE方法：基于集成学习，尝试将每个属性作为父属性来构建SPODE，然后将属性中非空个数>m'的那些SPODE集成。

## 贝叶斯网
亦称“信念网”，借助有向无环图来刻画属性之间的依赖关系，并使用条件概率表（对于连续属性则是条件概率密度函数）来描述属性的联合概率分布。主要是通过有向无环图连接有关联的属性。
$$
P_B(x1,x2,...,x_d) = \prod_{i=1}^d P_B(x_i|\pi_i) = \prod_{i=1}^d \theta_{x_i|\pi_i}
$$
例：
```
graph LR
x1-->x3
x1-->x4
x2-->x4
x2-->x5

```

$$
P(x_1,x_2,x_3,x_4,x_5) = P(x_1)P(x_2)P(x_3|x_1)P(x_4|x_1,x_2)P(x_5|x_2)
\\
x_3,x_4 \text{显然在给定}x_1\text{的取值时独立，记作}x_3 \perp x_4 | x_1
$$

**同父结构**：给定x1，则x3,x4条件独立
```
graph LR
x1-->x3
x1-->x4
```
**V型结构**：给定子节点x4，x1x2必不独立。而x4未知时，x1x2相互独立，称为“边际独立性”，记为$x_1 \perp\perp x_2$
```
graph LR
x1-->x4
x2-->x4
```
**顺序结构**：给定x，y与z条件独立
```
graph LR
z-->x
x-->y
```
为了分析图中的条件独立性，使用“有向分离”，产生“道德图”：
- 找出有向图中所有V型结构，在它们的两个父节点间加上一条无向边，”道德化“（孩子的父母应该建立牢靠的关系，否则不道德）
- 将所有有向边改为无向边

在图中，若变量x,y能被变量集合z分隔开，那么$x\perp y | z$
### 训练贝叶斯网
给定数据集D，贝叶斯网B=<G,θ>，|B|是贝叶斯网的参数个数，f(θ)表示描述每个参数θ所需的字节数，最小化评分函数：
$$
s(B|D) = f(\theta)|B| - LL(B|D)
\\
\text{其中B的对数似然为：}LL(B|D) = \sum_{i=1}^m log P_B(x_i)
$$
第一项是编码B所需字节数，第二项是计算B对应的概率分布PB需要多少字节来描述D
- **AIC**:f(θ)=1
- **BIC**:f(θ)=0.5logm

若贝叶斯网B=<G,θ>的网络结构G固定，那么评分函数第一项为常数，那么问题就变成极大似然估计。然而该问题是NP难的，因此要通过贪心法或一些启发式算法进行网络结构的搜索，而数据的评分采样则常用吉布斯采样、变分推断。

### EM算法
**隐变量Z**：变量中存在未被探测出的值。
$$
LL(\theta | X) = ln P(X|\theta) = ln \sum_{Z}P(X,Z|\theta)
$$
**EM算法**：只知道结果，不知道过程；求解样本属于哪个分布，这一个分布的参数是多少。若参数θ已知，可根据数据推断出最优隐变量Z的值，反之，若Z已知，可以对θ做最大似然估计。











