**无监督学习**：
- 聚类
- 密度估计 density estimation
- 异常检测 anomaly detection
# 聚类
提高簇内相似度，降低簇间相似度（与LDA思想类似，区别在于：聚类无监督、LDA只是线性关系）
## 性能度量
- 外部指标：专家的划分结果等
- 内部指标：直接考察聚类结果而不利用任何参考模型

对数据集D，有m个样本，假定通过聚类给出划分为C，参考模型给出划分为C\*，相应地令$\lambda$与$\lambda$\*分别表示与C,C\*对应的簇标记向量，定义如下式子：
$$
a = |SS|, SS = {(x_i,x_j)|\lambda_i=\lambda_j,\lambda_i^*=\lambda_j^*,i<j}
\\
b = |SD|, SS = {(x_i,x_j)|\lambda_i=\lambda_j,\lambda_i^* \ne \lambda_j^*,i<j}
\\
c = |DS|, SS = {(x_i,x_j)|\lambda_i \ne \lambda_j,\lambda_i^*=\lambda_j^*,i<j}
\\
d = |D|, SS = {(x_i,x_j)|\lambda_i \ne \lambda_j,\lambda_i^* \ne \lambda_j^*,i<j}
\\
avg(C):簇C内样本间平均距离
\\
diam(C):簇C内样本间最大距离
\\
d_{min}(C_i,C_j):簇C_i与簇C_j最近样本间的距离
\\
d_{cen}(C_i,C_j):簇C_i与簇C_j中心点间的距离
$$
因此有如下性能度量，前三个在[0,1]区间，值越大越好。DBI越小越好，DI越大越好
- Jaccard系数(Jaccard Coefficient ,简称JC)
$$
JC=\frac{a}{a+b+c}
$$
- FM 指数(Fowlkes and Mallows lndex，简称FMI)
$$
FMI=\sqrt{\frac{a}{a+b}\cdot \frac{a}{a+c}}
$$
- Rand 指数(Rand Index，简称RI)
$$
RI = \frac{2(a+d)}{m(m-1)}
$$
- DB 指数(Davies-Bouldin Index，简称DBI)
$$
DBI=\frac{1}{k}\sum_{i=1}^k \max_{j \ne i} (\frac{avg(C_i)+avg(C_j)}{d_{cen}(\mu_i,\mu_j)})
$$
- Dunn指数(Dunn Index，简称DI)
$$
DI = \min_{1 \leq i \leq k}\{ \min_{j\ne i}(\frac{d_{min}(C_i,C_j)}{\max_{1 \leq l \leq k} diam(C_l)}) \}
$$

## 距离计算

常用**"闵可夫斯基距离"** (Minkowski distance)，p为超参：
$$
dist_{mk}(x_i,x_j) = (\sum_{u=1}^n |x_{iu}-x_{ju}|^p)^{\frac{1}{p}}
$$
对于离散属性，可采用VDM (Value Difference Metric)，令$m_{u,a}$为属性u上取值为a的样本数，$m_{u,a,i}$表示第i个样本簇在属性u上取值为a的样本数，k为样本簇数，则属性u上两个离散值a与b之间的VDM距离为：
$$
VDM_p(a,b)=\sum_{i=1}^k |\frac{m_{u,a,i}}{m_{u,a}} - \frac{m_{u,b,i}}{m_{u,n}}|^p
$$
将离散和非离散的结合到一起，假设有$n_c$个有序属性，$n-n_c$个无序属性：
$$
MinkovDM_p(x_i,x_j) = (\sum_{u=1}^{n_c} |x_{iu}-x_{ju}|^p + \sum_{u=n_c+1}^n VDM_p(x_{iu},x_{ju}))^{\frac{1}{p}}
$$

## K-means

最小化簇内平方误差，其中$u_i$是簇$C_i$的均值向量：
$$
E=\sum_{i=1}^k \sum_{x \in C_i} ||x-u_i||_2^2
$$
每一个epoch中，根据上一轮的k个簇求簇中心点，再对每一个点找离它最近的中心点形成新的簇。













