

```python
输入：训练集D，属性集A
过程：TreeGenerate(D,A):
    生成节点node
    if D中样本全属于同一类别C:   ......................(1)
        将node标记为C类叶节点
        return
    if A=∅ or D中样本在A上取值相同：  .................(2)
        将node标记为叶节点，类别标记为D中样本数最多的类
        return
    从A中选择最优划分属性a*
    for a*的每个值av*:
        为node生成一个分支，令Dv表示D中在a*上取值为av*的样本子集。
        if Dv 为空:    ...............................(3)
            将分支节点标记为叶节点，类别标记为D中样本最多的类
            return
        else:
            以TreeGenerate(Dv,A\{a*})为分支节点
输出：以node为根节点的决策树
```

## 划分属性

### ID3
`信息熵`：度量样本集合纯度的指标，假定样本D中第k类样本所占比例为pk，则D的信息熵为：
$$
Ent(D) = -\sum_{k=1}^{|y|}{p_k\log_2p_k}
$$
信息熵越小，纯度越高（约定p=0时，Ent为0）

特征a有V个可能的取值{a1,a2,...,aV}，那么`信息增益`：
$$
Gain(D,a) = Ent(D) - \sum_{v=1}^V{\frac{|D^v|}{|D|}Ent(D^v)}
$$
信息增益越大，用属性a划分得到的“纯度提升”也越大，`ID3`就是以信息增益为划分标准。

### C4.5

`C4.5`算法采用`信息增益率`：
$$
Gain\_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}   \\

\text{其中 } IV = -\sum^{V}_{v=1}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|} \text{ 称为属性a的固有值 instrinsic value}
$$
一般可能取值数目V越大，IV的值也越大。因此`C4.5`对可能取值数目（特征unique）较少的特征有偏好，因此先从所有特征中找到信息增益高于平均水平的特征，再从中选择信息增益率最高的。

### 基尼指数

`CART`采用基尼指数（Gini index）来划分属性
$$
Gini(D) = \sum_{k=1}^{|y|}\sum_{k'\ne k}{p_k}{p_k'} = 1-\sum_{k=1}^{|y|}{p_k^2}
$$
Gini反映了从数据集D中随机抽取两个数，类别不一致的概率。因此Gini越小，数据集D的纯度越高。

因此以a属性来划分，它的Gini指数为
$$
Gini(D,a) = \sum_{v=1}^V{\frac{|D^v|}{|D|}Gini(D^v)}
$$
于是在候选属性集A中，选取划分后Gini指数最小的属性作为最优划分属性。

## 剪枝
`预剪枝`：在划分前估计该划分是否能带来决策树泛化能力提升，若不能，则该节点标记为叶节点。

`后剪枝`：自底向上对非叶节点进行考察，若能带来决策树泛化能力提升，则该子树替换为叶节点。

### 预剪枝
按某属性划分时，若该属性划分了x个叶节点，对每个叶节点，挑选正例最多的作为这个叶节点的类别，然后计算划分前后正确度是否提升，从而判定该节点是否应该划分。

## 连续与缺失值
以中位数作为划分值，将连续属性离散化。与离散属性不同的是，离散属性父节点用过之后，子节点不再使用。而连续属性则可以使用，如父节点a<=15，子节点还能a<=5

### C4.5
对于缺失值，在找划分点、求对某一属性的信息增益的时候，只计算该属性无空缺部分。在划分的时候，把缺失属性丢到所有子树中去，但把它的权重设为丢到这一子树中的概率。具体公式如下：
$$
\widetilde{D} \text{表示D在属性a上没有缺失值的样本子集} \\

\widetilde{D}_k \text{表示属于第k类的样本子集}\\

\widetilde{D}^v \text{表示在属性a上取值为} a^v \text{的样本子集}\\

\rho = \frac{\sum_{x\in \widetilde{D}} w_x}{\sum_{x\in D} w_x} \text{无缺失样本所占比例}\\

\widetilde{p}_k = \frac{\sum_{x\in \widetilde{D}_k} w_x}{\sum_{x\in \widetilde{D}} w_x} \text{无缺失样本中第k类所占比例}\\

\widetilde{r}^v = \frac{\sum_{x\in \widetilde{D}^v} w_x}{\sum_{x\in \widetilde{D}} w_x} \text{无缺失样本中属性a上取值} a^v \text{所占比例}\\

Gain(D,a) = \rho \times Gain(\widetilde{D},a) = \rho \times (Ent(\widetilde{D}) - \sum_{v=1}^V \tilde{r}_v Ent(\tilde{D}^v))\\

\text{其中} Ent(\tilde{D}^v) = -\sum^{|y|}_{k=1}\tilde{p}_klog_2\tilde{p}_k\\
$$

### CART
$$
Gini(D,a) = \rho \times \sum_{v=1}^V{\tilde{r}_v Gini(\tilde{D}^v)}\\

\text{其中} Gini(D) = 1-\sum_{k=1}^{|y|}{\tilde{p}_k^2}
$$


## 多变量决策树
普通决策树决策边界为与坐标轴平行的垂直直线，因此若是能采用斜的划分边界就能表示性更强。每一个非叶节点是形如$\sum^d_{i=1}w_i a_i = t$的线性分类器。



























