### 凸函数
对凸函数求导，当它的导数为0时，则得到最优解。

`凸函数`：对区间[a,b]上的函数f，若对区间中任意两点x1,x2均有$f(\frac{x_1+x_2}{2})\leq\frac{f(x_1)+f(x_2)}{2}$，则称f是区间[a,b]上的凸函数。

凸函数的判别：若二阶导数在区间上非负，则为凸函数。若严格恒大于0，则称严格凸函数。

### 线性判别分析
Linear Discriminant Analysis(LDA)

**思想**：将样例投影到一条直线上，并让同类样例的投影点尽可能接近，异类尽可能远离。

令$X_i,\mu_i,\Sigma_i$为数据集D上第i类示例的集合、均值向量、协方差矩阵，直线$\omega$。那么两类样本在直线上的投影分别为$\omega^T\mu_0$和$\omega^T\mu_1$，协方差分别为$\omega^T\Sigma_0\omega$和$\omega^T\Sigma_1\omega$，由于直线是一维空间，因此这些都是实数。那么问题就变成让同类的协方差尽可能小，让异类的中心距离尽可能大。

同时考虑二者，则可得到欲要最大化的目标：
$$
J=\frac{||\omega^T\mu_0-\omega^T\mu_1||^2_2}{\omega^T\Sigma_0\omega+\omega^T\Sigma_1\omega} = \frac{\omega^T(\mu_0-\mu_1)(\mu_0-\mu_1)^T\omega}{\omega^T(\Sigma_0+\Sigma_1)\omega}
$$
其中，协方差矩阵为
$$
\Sigma_i = \sum_{x\in X_i}(x-\mu_i)(x-\mu_i)^T
$$
令
$$
S_\omega = \Sigma_0+\Sigma_1 \\

S_b = (\mu_0-\mu_1)(\mu_0-\mu_1)^T

$$
不失一般性，令$\omega^TS_\omega\omega=1$，那么再用拉格朗日乘子法，解得
$$
\omega = S^{-1}_\omega(\mu_0-\mu_1)
$$
**多分类LDA**：可用作降维

### 多分类
包括OvO,OvR,MvM

MvM（多对多）：分为正反类对比，一共M次划分。常用*纠错输出码*(Error Correcting Output Codes, ECOC)、DAG拆分法。
- 编码：对N个类别划分M次，每次划分一部分为正类(+1)，一部分为反类(-1)，一部分不参与训练(0)，那么就是M个训练集训练出M个分类器。
- 解码：对M个分类器进行测试，并计算与各个类别的海明距离/欧氏距离。距离最近就属于哪个类别。

### 类别不平衡问题
线性分类器往往设定阈值0.5为分类的边界，但是实际应该是
$$
\frac{y'}{1-y'} = \frac{y}{1-y}*\frac{m^-}{m^+}
$$
，因此要用上再缩放（rescaling）。
- 下采样,downsampling（欠采样,undersampling）：去除一些反例使正反样例均衡。（使用多个学习器，每个学习器使用一部分的反例，最后ensemble，从而避免丢失重要信息）
- 上采样,upsampling（过采样，oversampling）：增加一些正例使正反样例均衡。（不能简单增加，而是通过对正例插值）
- 阈值移动：使用上面的公式，嵌入到决策过程中。






























