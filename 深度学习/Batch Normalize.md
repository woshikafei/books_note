# Batch Normalize

核心思想：对每一层进行归一化。
$$
\mu 为均值 \\
\sigma^2 为方差 \\
z_{norm} = \frac{z-\mu}{\sqrt{\sigma^2+\epsilon}}
$$
但是每一层有时有各自的分布特征，这也是一种特征，因此引入
$$
\hat{z} = \gamma z_{norm} + \beta
$$

- 进行缩放和平移（把$z_{norm}$代入，即相当于在合适的$\gamma,\beta$，仍然有分布特征），而它们又是要学习的参数，即又可以学习到最合适的分布特征。
- 对于某一层而言，它的前一层的大致分布不会打破（假设它的上一层就是数据，那么数据分布的变化会造成模型大幅变化。因此前一层大致稳定会使后面更快收敛）。
- 原有的脏数据在归一化之后影响变小
- 轻微增加噪音使模型更鲁棒
- 归一化的特性使其加速拟合
- 一定程度解决梯度消失/爆炸（否则$0.9^{30}$极小，很可能最后很多神经元死掉变为0）

**缺点**：

- 消除了数据中的绝对差距，对于图像中的某些问题可能使效果变差

### 测试时

方法（1）在训练时应该最后计算forward每一个minibatch的每一层的均值和方差，从而得到测试时使用的均值和方差。

方法（2）训练时使用指数加权平均计算。