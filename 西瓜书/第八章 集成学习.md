ensemble : 美 [ɑ:nˈsɑ:mbl]

## Boosting
`AdaBoost` : 每一轮根据样本分布为每个训练样本重新赋予权重。

对于每一个弱分类器m：
$$
\begin{align}
& 1.在上一步的训练样本权重下学习得到本次分类结果G_m(x)
\\
& 2.计算分类误差率e_m = \sum_{i=1}^N w_{m,i} P(G_m(x_i) \ne y_i)
\\
& 3.更新G_m在总分类器上的权重a_m = \frac{1}{2}\log\frac{1-e_m}{e_m}
\\
& 4.更新样本权重w_{m+1,i}=\frac{w_{m,i}}{z_m} e^{-a_my_iG_m(x_i)}
\\
& ,其中z_m为了归一化概率为1，z_m=\sum_{i=1}^N w_{m,i} e^{-a_my_iG_m(x_i)}
\\
& 5.第i次循环后，强学习器F(x_i)=sign(\sum_{i=1}^N a_mG_m(x_i))
\\
& ,其中F_m(x)=F_{m-1}(x)+a_mG_m(x)
\end{align}
$$
公式推导：
$$
\begin{align}
指数损失：L=\frac{1}{n}\sum_{i=1}^n e^{-y_if(x_i)},y_i=
	\begin{cases} 
	1 \\ -1
	\end{cases}
\\

\end{align}
$$

$$
\begin{align}
因此，Loss_m= & \frac{1}{N} \sum_{i=1}^N e^{-y_iF_m(x_i)}
\\
代入(7)式,= & \frac{1}{N} \sum_{i=1}^N e^{-y_i(f_{m-1}(x_i)+a_mG_m(x_i))}
\\
= & \frac{1}{N} \sum_{i=1}^N \widetilde{w_{m,i}} e^{-y_ia_mG_m(x_i)}
\\
= & \frac{1}{N} \sum_{y_i=G_m(x_i)}\widetilde{w_{m,i}}e^{a_m} + \sum_{y_i \neq G_m(x_i)}\widetilde{w_{m,i}}e^{a_m}
\\
代入(2)式中e_m， = & (1-e_m)e^{-a_m}+e_me^{a_m}
\\
所以\frac{\partial Loss}{\partial a_m} = & \frac{1}{2} log\frac{1-e_m}{e_m}
\\

其中\widetilde{w_{m,i}} & = e^{y_iF_{m-1}(x_i)}为常数项，可以任意提出
\\
& =e^{-y_i(F_{m-2}(x_i)+a_{m-1}G_{m-1}(x_i))}
\\
& = \widetilde{w_{m-1,i}} e^{-y_ia_{m-1}G_{m-1}(x_i)}
\end{align}
$$

所以公式推导部分的(14)式就是(3)式中的更新该弱分类器在总分类器上的权重，(17)是就是更新样本权重。









































